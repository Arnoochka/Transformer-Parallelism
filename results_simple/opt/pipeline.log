---device:1---:
OPTForCausalLM(
  (model): OPTModel(
    (decoder): OPTDecoder(
      (embed_tokens): PipeFakeModule(role=PipeRole.dummy
        (fake_module): FakeSeqModule(device=1)
      )
      (embed_positions): PipeFakeModule(role=PipeRole.dummy
        (fake_module): FakeSeqModule(device=1)
      )
      (final_layer_norm): PipeComputeModule(role=PipeRole.compute
        (module): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
      (layers): ModuleList(
        (0-10): 11 x PipeFakeModule(role=PipeRole.dummy
          (fake_module): FakeTupleSeqModule(device=1)
        )
        (11): PipeStrategyModule(role=PipeRole.recv
          (module): FakeTupleSeqModule(device=1)
          (strategy): LeaderTupleStrategyModule()
        )
        (12-23): 12 x PipeComputeModule(role=PipeRole.compute
          (module): OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=2048, out_features=8192, bias=True)
            (fc2): Linear(in_features=8192, out_features=2048, bias=True)
            (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
  )
  (lm_head): PipeStrategyModule(role=PipeRole.computeAndSend
    (module): Linear(in_features=2048, out_features=50272, bias=False)
    (strategy): LeaderStrategyModule()
  )
)
                                  time  ...  max_memory_gpu_1
name                                    ...                  
start                     13744.402329  ...          0.000003
generator                 13748.404122  ...          2.634754
max new tokens:256 start  13748.406341  ...          2.634755
max new tokens:256 stop   14399.644908  ...          3.391171

[4 rows x 5 columns]
-------Benchmark results-------
model name: my_opt-1.3b
model size: 4.902, dtype: float32
mean inference time (s): 651.239
mean throughput (token/s): 4.717
data size: 12, batch size: 12, max len: 64
max new tokens: [256]
    inference time (s): 651.239
    throughput (token/s): 4.717

max memory allocated per device for generator (GB):            
    device 0: 2.650
    device 1: 2.635

max memory allocated per device for inference (GB):            
    device 0: 3.407
    device 1: 3.391

max memory allocated per device (GB):            
    device 0: 3.407
    device 1: 3.391
---device:0---:
OPTForCausalLM(
  (model): OPTModel(
    (decoder): OPTDecoder(
      (embed_tokens): PipeComputeModule(role=PipeRole.compute
        (module): Embedding(50272, 2048, padding_idx=1)
      )
      (embed_positions): PipeComputeModule(role=PipeRole.compute
        (module): OPTLearnedPositionalEmbedding(2050, 2048)
      )
      (final_layer_norm): PipeFakeModule(role=PipeRole.dummy
        (fake_module): FakeTupleSeqModule(device=0)
      )
      (layers): ModuleList(
        (0-10): 11 x PipeComputeModule(role=PipeRole.compute
          (module): OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=2048, out_features=8192, bias=True)
            (fc2): Linear(in_features=8192, out_features=2048, bias=True)
            (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
          )
        )
        (11): PipeStrategyModule(role=PipeRole.computeAndSend
          (module): OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
              (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=2048, out_features=8192, bias=True)
            (fc2): Linear(in_features=8192, out_features=2048, bias=True)
            (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
          )
          (strategy): LeaderTupleStrategyModule()
        )
        (12-23): 12 x PipeFakeModule(role=PipeRole.dummy
          (fake_module): FakeTupleSeqModule(device=0)
        )
      )
    )
  )
  (lm_head): PipeStrategyModule(role=PipeRole.recv
    (module): FakeSeqModule(device=0)
    (strategy): LeaderStrategyModule()
  )
)

WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/victor/anaconda3/envs/nir/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/victor/anaconda3/envs/nir/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
start benchmark
/home/victor/anaconda3/envs/nir/lib/python3.11/site-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
/home/victor/anaconda3/envs/nir/lib/python3.11/site-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
model:
[OPTModel(
  (decoder): OPTDecoder(
    (embed_tokens): TPColumnEmbedding()
    (embed_positions): SimpleSplitter(
      (module): OPTLearnedPositionalEmbedding(2050, 2048)
    )
    (final_layer_norm): TPLayerNorm()
    (layers): ModuleList(
      (0-23): 24 x OPTDecoderLayer(
        (self_attn): OPTAttention(
          (k_proj): TPRowLinear()
          (v_proj): TPRowLinear()
          (q_proj): TPRowLinear()
          (out_proj): TPColumnLinear()
        )
        (activation_fn): ReLU()
        (self_attn_layer_norm): TPLayerNorm()
        (fc1): TPRowLinear()
        (fc2): TPColumnLinear()
        (final_layer_norm): TPLayerNorm()
      )
    )
  )
), TPRowLinear()]
/home/victor/anaconda3/envs/nir/lib/python3.11/site-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
/home/victor/anaconda3/envs/nir/lib/python3.11/site-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
stop benchmark
                               time  ...  max_memory_gpu_1
name                                 ...                  
start                   4596.515349  ...          0.000003
generator               4602.972971  ...          2.651028
max new tokens:8 start  4602.974815  ...          2.651029
max new tokens:8 stop   4627.163376  ...          2.825314

[4 rows x 5 columns]
-------Benchmark results-------
model name: my_opt-1.3b
model size: 4.902, dtype: float32
mean inference time (s): 24.189
mean throughput (token/s): 3.969
data size: 12, batch size: 12, max len: 64
max new tokens: [8]
    inference time (s): 24.189
    throughput (token/s): 3.969

max memory allocated per device for generator (GB):            
    device 0: 2.651
    device 1: 2.651

max memory allocated per device for inference (GB):            
    device 0: 2.825
    device 1: 2.825

max memory allocated per device (GB):            
    device 0: 2.825
    device 1: 2.825

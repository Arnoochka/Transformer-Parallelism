[2025-09-17 19:34:02,627] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-17 19:34:05,812] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-17 19:34:06,479] [WARNING] [runner.py:220:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-09-17 19:34:06,480] [INFO] [runner.py:610:main] cmd = /home/victor/anaconda3/envs/nir/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None opt_my.py
[2025-09-17 19:34:08,536] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-17 19:34:11,610] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-17 19:34:12,273] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-09-17 19:34:12,273] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-09-17 19:34:12,273] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-09-17 19:34:12,273] [INFO] [launch.py:164:main] dist_world_size=2
[2025-09-17 19:34:12,273] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-09-17 19:34:12,274] [INFO] [launch.py:256:main] process 5153 spawned with command: ['/home/victor/anaconda3/envs/nir/bin/python', '-u', 'opt_my.py', '--local_rank=0']
[2025-09-17 19:34:12,276] [INFO] [launch.py:256:main] process 5154 spawned with command: ['/home/victor/anaconda3/envs/nir/bin/python', '-u', 'opt_my.py', '--local_rank=1']
/home/victor/anaconda3/envs/nir/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/victor/anaconda3/envs/nir/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
start benchmark
/home/victor/anaconda3/envs/nir/lib/python3.11/site-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
/home/victor/anaconda3/envs/nir/lib/python3.11/site-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
/home/victor/anaconda3/envs/nir/lib/python3.11/site-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
/home/victor/anaconda3/envs/nir/lib/python3.11/site-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
stop benchmark
output: ["is curly maple a hardwood\nIt's a hardwood. It's a hardwood that's hard to cut.", 'what is gingerbread with architectural\n\nwhat is gingerbread with architectural\n\nWhat is Gingerbread with Architectural?\n\nWhat is Gingerbread with Architectural?\n\nWhat is Gingerbread with Architectural?\n\nWhat is Gingerbread with Architectural?\n\nWhat is Gingerbread with Architectural?\n\nWhat is Gingerbread with Architectural?\n\nWhat is Gingerbread with Architectural?\n\nWhat is Gingerbread with Architectural?\n\nWhat is Gingerbread with Architectural?\n\nWhat is Gingerbread with Architectural?\n\nWhat is Gingerbread with Architectural?\n\nWhat is Gingerbread with Architectural?\n\nWhat is Gingerbread with Architectural?\n\nWhat is Gingerbread with Architectural?\n\nWhat is Gingerbread with Architectural?\n\nWhat is Gingerbread with Architectural?\n\nWhat is Gingerbread with Architectural?\n\nWhat is Gingerbread with Architectural?\n\nWhat is Gingerbread with Architectural?\n\nWhat is Gingerbread with Architectural?\n\nWhat is Gingerbread with Architectural?\n\nWhat is Gingerbread with Architectural?\n\nWhat is Gingerbread with Architectural?\n\nWhat is Gingerbread with Architectural?\n\nWhat is Gingerbread with Architectural', 'automaticall start skype\n\nI have a problem with my skype. I have a problem with my skype. I have a problem with my skype. I have a problem with my skype. I have a problem with my skype. I have a problem with my skype. I have a problem with my skype. I have a problem with my skype. I have a problem with my skype. I have a problem with my skype. I have a problem with my skype. I have a problem with my skype. I have a problem with my skype. I have a problem with my skype. I have a problem with my skype. I have a problem with my skype. I have a problem with my skype. I have a problem with my skype. I have a problem with my skype. I have a problem with my skype. I have a problem with my skype. I have a problem with my skype. I have a problem with my skype. I have a problem with my skype. I have a problem with my skype. I have a problem with my skype. I have a problem with my skype. I have a problem with my skype. I have a']
                     time  memory_gpu_0  ...  memory_gpu_1  max_memory_gpu_1
name                                     ...                                
start         6222.030130  4.768372e-07  ...  4.768372e-07          0.000003
generator     6229.170025  2.650941e+00  ...  2.650941e+00          2.650944
loop:0 start  6229.171231  2.650942e+00  ...  2.650942e+00          2.650945
loop:0 stop   6986.612391  2.658915e+00  ...  2.658915e+00          4.330725

[4 rows x 5 columns]
-------Benchmark results-------
model name: my_opt-1.3b
model size: 4.902, dtype: float32
mean inference time (s): 757.441
mean throughput (token/s): 3.718
num loops: 1, data size: 11, batch size: 11, max len: 64, max new tokens: 256

inference time (s): 757.441
throughput (token/s): 3.718

max memory allocated per device for generator (GB):            
    device 0: 2.651
    device 1: 2.651

max memory allocated per device for inference (GB):            
    device 0: 4.331
    device 1: 4.331

max memory allocated per device (GB):            
    device 0: 4.331
    device 1: 4.331
[2025-09-17 19:47:11,627] [INFO] [launch.py:351:main] Process 5153 exits successfully.
[2025-09-17 19:47:11,627] [INFO] [launch.py:351:main] Process 5154 exits successfully.

[OPTModel(
  (decoder): OPTDecoder(
    (embed_tokens): TPColumnEmbedding()
    (embed_positions): (module): OPTLearnedPositionalEmbedding(2050, 2048)
    (final_layer_norm): LayerNorm()
    (layers): ModuleList(
      (0-23): 24 x OPTDecoderLayer(
        (self_attn): OPTAttention(
          (k_proj): TPColumnLinear()
          (v_proj): TPColumnLinear()
          (q_proj): TPColumnLinear()
          (out_proj): TPColumnLinear()
        )
        (activation_fn): ReLU()
        (self_attn_layer_norm): LayerNorm()
        (fc1): TPColumnLinear()
        (fc2): TPRowLinear()
        (final_layer_norm): LayerNorm()
      )
    )
  )
), TPÐ¡Linear()]

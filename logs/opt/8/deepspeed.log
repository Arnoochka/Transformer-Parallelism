WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2025-10-24 21:26:19,742] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-24 21:26:19,829] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-24 21:26:24,385] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-24 21:26:24,392] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-10-24 21:26:24,433] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-24 21:26:24,440] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-10-24 21:26:24,440] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/home/victor/anaconda3/envs/nir/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/victor/anaconda3/envs/nir/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
start benchmark
/home/victor/anaconda3/envs/nir/lib/python3.11/site-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
/home/victor/anaconda3/envs/nir/lib/python3.11/site-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
[2025-10-24 21:26:33,774] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.17.5, git-hash=unknown, git-branch=unknown
[2025-10-24 21:26:33,775] [INFO] [logging.py:107:log_dist] [Rank 0] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-24 21:26:33,775] [INFO] [logging.py:107:log_dist] [Rank 0] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1
[2025-10-24 21:26:33,851] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 2048, 'intermediate_size': 8192, 'heads': 32, 'num_hidden_layers': -1, 'dtype': torch.float32, 'pre_layer_norm': True, 'norm_type': <NormType.LayerNorm: 1>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 2, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': -1, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.ReLU: 2>, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 1024, 'min_out_tokens': 1, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False, 'use_triton': False, 'triton_autotune': False, 'num_kv': -1, 'rope_theta': 10000, 'invert_mask': True}
Using /home/victor/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/victor/.cache/torch_extensions/py311_cu118/transformer_inference/build.ninja...
Using /home/victor/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Building extension module transformer_inference...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module transformer_inference...
Time to load transformer_inference op: 0.01548314094543457 seconds
Loading extension module transformer_inference...
Time to load transformer_inference op: 0.10528993606567383 seconds
/home/victor/anaconda3/envs/nir/lib/python3.11/site-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
model:
[OPTModel(
  (decoder): OPTDecoder(
    (embed_tokens): Embedding(50272, 2048, padding_idx=1)
    (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)
    (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0-23): 24 x DeepSpeedOPTInference(
        (attention): DeepSpeedSelfAttention(
          (qkv_func): QKVGemmOp()
          (score_context_func): SoftmaxContextOp()
          (linear_func): LinearOp()
          (vector_matmul_func): VectorMatMulOp()
        )
        (mlp): DeepSpeedMLP(
          (mlp_gemm_func): MLPGemmOp(
            (pre_rms_norm): PreRMSNormOp()
          )
          (vector_matmul_func): VectorMatMulOp()
          (fused_gemm_gelu): GELUGemmOp()
          (residual_add_func): ResidualAddOp(
            (vector_add): VectorAddOp()
          )
        )
        (layer_norm): LayerNormOp()
      )
    )
  )
), Linear(in_features=2048, out_features=50272, bias=False)]
------------------------------------------------------
Free memory : 4.594910 (GigaBytes)  
Total memory: 7.919250 (GigaBytes)  
Requested memory: 3.937500 (GigaBytes) 
Setting maximum total tokens (input + output) to 1024 
WorkSpace: 0x7da880000000 
------------------------------------------------------
stop benchmark
                               time  ...  max_memory_gpu_1
name                                 ...                  
start                   5151.940439  ...          0.000003
generator               5169.058243  ...          2.705658
max new tokens:8 start  5169.060391  ...          2.705659
max new tokens:8 stop   5176.567062  ...          2.882538

[4 rows x 5 columns]
-------Benchmark results-------
model name: deepspeed_opt-1.3b
model size: 4.902, dtype: float32
mean inference time (s): 7.507
mean throughput (token/s): 12.789
data size: 12, batch size: 12, max len: 64
max new tokens: [8]
    inference time (s): 7.507
    throughput (token/s): 12.789

max memory allocated per device for generator (GB):            
    device 0: 2.706
    device 1: 2.706

max memory allocated per device for inference (GB):            
    device 0: 2.883
    device 1: 2.883

max memory allocated per device (GB):            
    device 0: 2.883
    device 1: 2.883
